{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gensim\n\nmodel = gensim.models.KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-17T01:33:46.436558Z","iopub.execute_input":"2023-01-17T01:33:46.437053Z","iopub.status.idle":"2023-01-17T01:34:23.245302Z","shell.execute_reply.started":"2023-01-17T01:33:46.437011Z","shell.execute_reply":"2023-01-17T01:34:23.244074Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import re\nimport codecs\n\ndef preprocess_text(text):\n    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n    text = re.sub(' +', ' ', text)\n    return text.strip()\n\ndef prepare_for_w2v(filename_from, filename_to, lang):\n    raw_text = codecs.open(filename_from, \"r\", encoding='windows-1251').read()\n    with open(filename_to, 'w', encoding='utf-8') as f:\n        for sentence in nltk.sent_tokenize(raw_text, lang):\n            print(preprocess_text(sentence.lower()), file=f)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:34:23.247383Z","iopub.execute_input":"2023-01-17T01:34:23.247713Z","iopub.status.idle":"2023-01-17T01:34:23.305289Z","shell.execute_reply.started":"2023-01-17T01:34:23.247681Z","shell.execute_reply":"2023-01-17T01:34:23.304305Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"On the Word2Vec training stage the following hyperparameters were used:\n- Dimensionality of the feature vector is 200.\n- The maximum distance between analyzed words within a sentence is 5.\n- Ignores all words with the total frequency lower than 5 per corpus.","metadata":{}},{"cell_type":"code","source":"import multiprocessing\nfrom gensim.models import Word2Vec\n\n\ndef train_word2vec(filename):\n    data = gensim.models.word2vec.LineSentence(filename)\n    return Word2Vec(data, size=200, window=5, min_count=5, workers=multiprocessing.cpu_count())","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:34:23.306682Z","iopub.execute_input":"2023-01-17T01:34:23.307793Z","iopub.status.idle":"2023-01-17T01:34:23.314580Z","shell.execute_reply.started":"2023-01-17T01:34:23.307754Z","shell.execute_reply":"2023-01-17T01:34:23.313680Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"keys = ['Paris', 'Python', 'Sunday', 'Tolstoy', 'Twitter', 'bachelor', 'delivery', 'election', 'expensive',\n        'experience', 'financial', 'food', 'iOS', 'peace', 'release', 'war']\n\nembedding_clusters = []\nword_clusters = []\nfor word in keys:\n    embeddings = []\n    words = []\n    for similar_word, _ in model.most_similar(word, topn=30):\n        words.append(similar_word)\n        embeddings.append(model[similar_word])\n    embedding_clusters.append(embeddings)\n    word_clusters.append(words)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:36:08.420739Z","iopub.execute_input":"2023-01-17T01:36:08.422026Z","iopub.status.idle":"2023-01-17T01:36:13.036815Z","shell.execute_reply.started":"2023-01-17T01:36:08.421980Z","shell.execute_reply":"2023-01-17T01:36:13.034994Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Next, we proceed to the fascinating part of this paper, the configuration of t-SNE. In this section, we should pay our attention to the following hyperparameters.\n\n- The number of components, i.e. the dimension of the output space.\n- Perplexity value, which in the context of t-SNE, may be viewed as a smooth measure of the effective number of neighbors. It is related to the number of nearest neighbors that are employed in many other manifold learners (see the picture above). According to [1], it is recommended to select a value between 5 and 50.\n- The type of initial initialization for embeddings.","metadata":{}},{"cell_type":"code","source":"tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\nembedding_clusters = np.array(embedding_clusters)\nn, m, k = embedding_clusters.shape\nembeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:34:29.763669Z","iopub.execute_input":"2023-01-17T01:34:29.764302Z","iopub.status.idle":"2023-01-17T01:34:29.792586Z","shell.execute_reply.started":"2023-01-17T01:34:29.764254Z","shell.execute_reply":"2023-01-17T01:34:29.790946Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/1290181606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsne_model_en_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_clusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membeddings_en_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsne_model_en_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_clusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TSNE' is not defined"],"ename":"NameError","evalue":"name 'TSNE' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n% matplotlib inline\n\n\ndef tsne_plot_similar_words(labels, embedding_clusters, word_clusters, a=0.7):\n    plt.figure(figsize=(16, 9))\n    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n        x = embeddings[:,0]\n        y = embeddings[:,1]\n        plt.scatter(x, y, c=color, alpha=a, label=label)\n        for i, word in enumerate(words):\n            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2), \n                         textcoords='offset points', ha='right', va='bottom', size=8)\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.savefig(\"f/г.png\", format='png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\ntsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:35:05.375154Z","iopub.execute_input":"2023-01-17T01:35:05.375587Z","iopub.status.idle":"2023-01-17T01:35:05.786116Z","shell.execute_reply.started":"2023-01-17T01:35:05.375552Z","shell.execute_reply":"2023-01-17T01:35:05.784309Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"UsageError: Line magic function `%` not found.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In some cases, it can be useful to plot all word vectors at once in order to see the whole picture. Let us now analyze Anna Karenina, an epic novel of passion, intrigue, tragedy, and redemption.","metadata":{}},{"cell_type":"code","source":"prepare_for_w2v('data/Anna Karenina by Leo Tolstoy (ru).txt', 'train_anna_karenina_ru.txt', 'russian')\nmodel_ak = train_word2vec('train_anna_karenina_ru.txt')\n\nwords = []\nembeddings = []\nfor word in list(model_ak.wv.vocab):\n    embeddings.append(model_ak.wv[word])\n    words.append(word)\n    \ntsne_ak_2d = TSNE(n_components=2, init='pca', n_iter=3500, random_state=32)\nembeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:34:29.796142Z","iopub.status.idle":"2023-01-17T01:34:29.796813Z","shell.execute_reply.started":"2023-01-17T01:34:29.796615Z","shell.execute_reply":"2023-01-17T01:34:29.796636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tsne_plot_2d(label, embeddings, words=[], a=1):\n    plt.figure(figsize=(16, 9))\n    colors = cm.rainbow(np.linspace(0, 1, 1))\n    x = embeddings[:,0]\n    y = embeddings[:,1]\n    plt.scatter(x, y, c=colors, alpha=a, label=label)\n    for i, word in enumerate(words):\n        plt.annotate(word, alpha=0.3, xy=(x[i], y[i]), xytext=(5, 2), \n                     textcoords='offset points', ha='right', va='bottom', size=10)\n    plt.legend(loc=4)\n    plt.grid(True)\n    plt.savefig(\"hhh.png\", format='png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\ntsne_plot_2d('Anna Karenina by Leo Tolstoy', embeddings_ak_2d, a=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:34:29.798013Z","iopub.status.idle":"2023-01-17T01:34:29.798667Z","shell.execute_reply.started":"2023-01-17T01:34:29.798466Z","shell.execute_reply":"2023-01-17T01:34:29.798486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The whole picture can be even more informative if we map initial embeddings in 3D space. In this time let us have a look at War and Peace, one of the vital novel of world literature and one of Tolstoy’s greatest literary achievements.","metadata":{}},{"cell_type":"code","source":"prepare_for_w2v('data/War and Peace by Leo Tolstoy (ru).txt', 'train_war_and_peace_ru.txt', 'russian')\nmodel_wp = train_word2vec('train_war_and_peace_ru.txt')\n\nwords_wp = []\nembeddings_wp = []\nfor word in list(model_wp.wv.vocab):\n    embeddings_wp.append(model_wp.wv[word])\n    words_wp.append(word)\n    \ntsne_wp_3d = TSNE(perplexity=30, n_components=3, init='pca', n_iter=3500, random_state=12)\nembeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:34:29.799863Z","iopub.status.idle":"2023-01-17T01:34:29.800533Z","shell.execute_reply.started":"2023-01-17T01:34:29.800306Z","shell.execute_reply":"2023-01-17T01:34:29.800334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\n\ndef tsne_plot_3d(title, label, embeddings, a=1):\n    fig = plt.figure()\n    ax = Axes3D(fig)\n    colors = cm.rainbow(np.linspace(0, 1, 1))\n    plt.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], c=colors, alpha=a, label=label)\n    plt.legend(loc=4)\n    plt.title(title)\n    plt.show()\n\n\ntsne_plot_3d('Visualizing Embeddings using t-SNE', 'War and Peace', embeddings_wp_3d, a=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T01:34:29.801715Z","iopub.status.idle":"2023-01-17T01:34:29.802108Z","shell.execute_reply.started":"2023-01-17T01:34:29.801908Z","shell.execute_reply":"2023-01-17T01:34:29.801926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is what texts look like from the Word2Vec and t-SNE prospective. We plotted a quite informative chart for similar words from Google News and two diagrams for Tolstoy’s novels. Also, one more thing, GIFs! GIFs are awesome, but plotting GIFs is almost the same as plotting regular graphs. So, I decided not to mention them in the article, but you can find the code for the generation of animations in the sources.\n","metadata":{}}]}